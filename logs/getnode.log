2025-05-18 18:33:01,235 - getnode - INFO - main.py:25 - === 开始执行爬虫任务 ===
2025-05-18 18:33:09,925 - getnode - INFO - crawler.py:122 - 跳过仓库：66个
2025-05-18 18:33:09,926 - getnode - INFO - main.py:30 - 发现 54 个相关仓库
2025-05-18 18:33:09,926 - getnode - INFO - main.py:34 - 开始收集节点文件...
2025-05-18 18:33:42,991 - getnode - INFO - crawler.py:56 - 已使用API次数: 100/小时
2025-05-18 18:34:17,846 - getnode - INFO - crawler.py:56 - 已使用API次数: 200/小时
2025-05-18 18:34:54,682 - getnode - INFO - crawler.py:56 - 已使用API次数: 300/小时
2025-05-18 18:35:31,482 - getnode - INFO - crawler.py:56 - 已使用API次数: 400/小时
2025-05-18 18:35:37,860 - getnode - INFO - main.py:38 - 总共发现 150 个节点文件
2025-05-18 18:35:37,860 - getnode - INFO - nodesjob.py:21 - 开始处理链接集合，共 150 个链接
2025-05-18 18:37:44,101 - getnode - ERROR - nodesjob.py:73 - 处理链接时发生异常: 处理异常: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out.
Traceback (most recent call last):
  File "D:\Learning\getnode\.venv\Lib\site-packages\urllib3\response.py", line 754, in _error_catcher
    yield
  File "D:\Learning\getnode\.venv\Lib\site-packages\urllib3\response.py", line 879, in _raw_read
    data = self._fp_read(amt, read1=read1) if not fp_closed else b""
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Learning\getnode\.venv\Lib\site-packages\urllib3\response.py", line 862, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\http\client.py", line 479, in read
    s = self.fp.read(amt)
        ^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\ssl.py", line 1252, in recv_into
    return self.read(nbytes, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\ssl.py", line 1104, in read
    return self._sslobj.read(len, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Learning\getnode\.venv\Lib\site-packages\requests\models.py", line 820, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "D:\Learning\getnode\.venv\Lib\site-packages\urllib3\response.py", line 1066, in stream
    data = self.read(amt=amt, decode_content=decode_content)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Learning\getnode\.venv\Lib\site-packages\urllib3\response.py", line 955, in read
    data = self._raw_read(amt)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\Learning\getnode\.venv\Lib\site-packages\urllib3\response.py", line 878, in _raw_read
    with self._error_catcher():
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "D:\Learning\getnode\.venv\Lib\site-packages\urllib3\response.py", line 759, in _error_catcher
    raise ReadTimeoutError(self._pool, None, "Read timed out.") from e  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "d:\Learning\getnode\src\nodesjob.py", line 38, in parse_node_links
    response = requests.get(url, timeout=15)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Learning\getnode\.venv\Lib\site-packages\requests\api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Learning\getnode\.venv\Lib\site-packages\requests\api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Learning\getnode\.venv\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Learning\getnode\.venv\Lib\site-packages\requests\sessions.py", line 746, in send
    r.content
  File "D:\Learning\getnode\.venv\Lib\site-packages\requests\models.py", line 902, in content
    self._content = b"".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b""
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Learning\getnode\.venv\Lib\site-packages\requests\models.py", line 826, in generate
    raise ConnectionError(e)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out.
2025-05-18 18:39:32,961 - getnode - INFO - nodesjob.py:78 - 链接处理完成。成功: 110, 失败: 40
2025-05-18 18:39:39,679 - getnode - INFO - history_manager.py:23 - 成功加载历史节点数量: 16617
2025-05-18 18:39:39,679 - getnode - ERROR - nodesjob.py:183 - 解析Clash配置文件时发生未知错误: 无效的配置类型
Traceback (most recent call last):
  File "d:\Learning\getnode\src\nodesjob.py", line 146, in _parse_clash_config_content
    raise ValueError("无效的配置类型")
ValueError: 无效的配置类型
2025-05-18 18:39:39,679 - getnode - INFO - history_manager.py:44 - 合并后总节点数: 16692
2025-05-18 18:39:39,679 - getnode - INFO - nodesjob.py:593 - 开始保存节点结果到目录: output
2025-05-18 18:39:39,813 - getnode - INFO - nodesjob.py:611 - 开始写入输出文件...
2025-05-18 18:39:46,266 - getnode - INFO - nodesjob.py:620 - 成功生成订阅文件，总节点数: 16687
2025-05-18 18:39:46,266 - getnode - INFO - tester.py:74 - === 批量测试开始 | 总节点数: 16692 ===
2025-05-18 18:40:26,101 - getnode - INFO - tester.py:105 - === 批量测试完成 | 有效节点: 13949 (成功率: 83.6%) ===
2025-05-18 18:40:26,101 - getnode - INFO - nodesjob.py:593 - 开始保存节点结果到目录: speedtest
2025-05-18 18:40:26,212 - getnode - INFO - nodesjob.py:611 - 开始写入输出文件...
2025-05-18 18:40:31,238 - getnode - INFO - nodesjob.py:620 - 成功生成订阅文件，总节点数: 13944
2025-05-18 18:40:31,317 - getnode - INFO - main.py:81 - 
=== 文件处理统计 ===
• 扫描文件总数: 2947
• 因大小跳过:   76 (2.6%)
• 有效处理文件: 2871
=== 节点处理统计 ===
• 扫描节点总数: 49866
• 节点去重数:   33174
• 真实节点数:   16692
2025-05-18 18:47:57,169 - getnode - INFO - main.py:25 - === 开始执行爬虫任务 ===
2025-05-18 18:48:14,051 - getnode - INFO - crawler.py:133 - 仓库搜索完成 | 总扫描仓库: 233 有效仓库: 120 跳过: 113
2025-05-18 18:48:14,051 - getnode - INFO - main.py:30 - 发现 120 个相关仓库
2025-05-18 18:48:14,051 - getnode - INFO - main.py:34 - 开始收集节点文件...
2025-05-18 18:48:52,927 - getnode - INFO - crawler.py:56 - 已使用API次数: 100/小时
2025-05-18 18:49:28,960 - getnode - INFO - crawler.py:56 - 已使用API次数: 200/小时
2025-05-18 18:50:06,497 - getnode - INFO - crawler.py:56 - 已使用API次数: 300/小时
2025-05-18 18:50:43,594 - getnode - INFO - crawler.py:56 - 已使用API次数: 400/小时
2025-05-18 18:51:20,477 - getnode - INFO - crawler.py:56 - 已使用API次数: 500/小时
2025-05-18 18:51:56,107 - getnode - INFO - crawler.py:56 - 已使用API次数: 600/小时
2025-05-18 18:52:31,467 - getnode - INFO - crawler.py:56 - 已使用API次数: 700/小时
2025-05-18 18:53:07,111 - getnode - INFO - crawler.py:56 - 已使用API次数: 800/小时
2025-05-18 18:53:46,447 - getnode - INFO - crawler.py:56 - 已使用API次数: 900/小时
2025-05-18 18:54:24,275 - getnode - INFO - crawler.py:51 - 已使用API次数: 1000/小时
2025-05-18 18:54:58,751 - getnode - INFO - crawler.py:56 - 已使用API次数: 1100/小时
2025-05-18 18:55:33,614 - getnode - INFO - crawler.py:56 - 已使用API次数: 1200/小时
2025-05-18 18:56:09,302 - getnode - INFO - crawler.py:56 - 已使用API次数: 1300/小时
2025-05-18 18:56:44,312 - getnode - INFO - crawler.py:56 - 已使用API次数: 1400/小时
2025-05-18 18:57:20,403 - getnode - INFO - crawler.py:56 - 已使用API次数: 1500/小时
2025-05-18 18:57:56,037 - getnode - INFO - crawler.py:56 - 已使用API次数: 1600/小时
2025-05-18 18:58:38,146 - getnode - INFO - crawler.py:56 - 已使用API次数: 1700/小时
2025-05-18 18:59:19,918 - getnode - INFO - crawler.py:56 - 已使用API次数: 1800/小时
2025-05-18 18:59:20,649 - getnode - INFO - main.py:38 - 总共发现 105 个节点文件
2025-05-18 18:59:20,649 - getnode - INFO - nodesjob.py:21 - 开始处理链接集合，共 105 个链接
2025-05-18 19:00:23,004 - getnode - INFO - nodesjob.py:78 - 链接处理完成。成功: 52, 失败: 53
2025-05-18 19:00:28,079 - getnode - INFO - history_manager.py:23 - 成功加载历史节点数量: 16692
2025-05-18 19:00:28,079 - getnode - ERROR - nodesjob.py:183 - 解析Clash配置文件时发生未知错误: 无效的配置类型
Traceback (most recent call last):
  File "d:\Learning\getnode\src\nodesjob.py", line 146, in _parse_clash_config_content
    raise ValueError("无效的配置类型")
ValueError: 无效的配置类型
2025-05-18 19:00:28,079 - getnode - INFO - history_manager.py:44 - 合并后总节点数: 11378
2025-05-18 19:00:28,079 - getnode - INFO - nodesjob.py:593 - 开始保存节点结果到目录: output
2025-05-18 19:00:28,181 - getnode - INFO - nodesjob.py:611 - 开始写入输出文件...
2025-05-18 19:00:32,029 - getnode - INFO - nodesjob.py:620 - 成功生成订阅文件，总节点数: 11224
2025-05-18 19:00:32,029 - getnode - INFO - tester.py:74 - === 批量测试开始 | 总节点数: 11378 ===
2025-05-18 19:01:11,369 - getnode - INFO - tester.py:105 - === 批量测试完成 | 有效节点: 11374 (成功率: 100.0%) ===
2025-05-18 19:01:11,369 - getnode - INFO - nodesjob.py:593 - 开始保存节点结果到目录: speedtest
2025-05-18 19:01:11,469 - getnode - INFO - nodesjob.py:611 - 开始写入输出文件...
2025-05-18 19:01:15,366 - getnode - INFO - nodesjob.py:620 - 成功生成订阅文件，总节点数: 11220
2025-05-18 19:01:15,584 - getnode - INFO - main.py:81 - 
=== 文件处理统计 ===
• 扫描文件总数: 13014
• 因大小跳过:   127 (1.0%)
• 有效处理文件: 12887
=== 节点处理统计 ===
• 扫描节点总数: 13313
• 节点去重数:   1935
• 真实节点数:   11378
2025-05-18 21:19:47,239 - getnode - INFO - main.py:25 - === 开始执行爬虫任务 ===
2025-05-18 21:20:00,453 - getnode - INFO - crawler.py:133 - 仓库搜索完成 | 总扫描仓库: 160 有效仓库: 60 跳过: 100
2025-05-18 21:20:00,453 - getnode - INFO - main.py:30 - 发现 60 个相关仓库
2025-05-18 21:20:00,453 - getnode - INFO - main.py:34 - 开始收集节点文件...
2025-05-18 21:20:36,271 - getnode - INFO - crawler.py:56 - 已使用API次数: 100/小时
2025-05-18 21:21:16,725 - getnode - INFO - crawler.py:56 - 已使用API次数: 200/小时
2025-05-18 21:21:53,775 - getnode - INFO - crawler.py:56 - 已使用API次数: 300/小时
2025-05-18 21:22:31,229 - getnode - INFO - crawler.py:56 - 已使用API次数: 400/小时
2025-05-18 21:23:00,075 - getnode - INFO - main.py:38 - 总共发现 151 个节点文件
2025-05-18 21:23:00,076 - getnode - INFO - nodesjob.py:21 - 开始处理链接集合，共 151 个链接
2025-05-18 21:24:40,479 - getnode - INFO - nodesjob.py:78 - 链接处理完成。成功: 114, 失败: 37
2025-05-18 21:24:43,687 - getnode - INFO - history_manager.py:23 - 成功加载历史节点数量: 11378
2025-05-18 21:24:44,993 - getnode - INFO - history_manager.py:34 - 新节点：16629,
历史节点：11378
2025-05-18 21:24:45,206 - getnode - INFO - history_manager.py:53 - 合并后总节点数: 27367
2025-05-18 21:24:45,206 - getnode - INFO - nodesjob.py:591 - 开始保存节点结果到目录: output
2025-05-18 21:24:45,405 - getnode - INFO - nodesjob.py:609 - 开始写入输出文件...
2025-05-18 21:24:55,299 - getnode - INFO - nodesjob.py:618 - 成功生成订阅文件，总节点数: 27208
2025-05-18 21:24:55,305 - getnode - INFO - tester.py:74 - === 批量测试开始 | 总节点数: 27367 ===
2025-05-18 21:26:18,212 - getnode - INFO - tester.py:105 - === 批量测试完成 | 有效节点: 13964 (成功率: 51.0%) ===
2025-05-18 21:26:18,212 - getnode - INFO - nodesjob.py:591 - 开始保存节点结果到目录: speedtest
2025-05-18 21:26:18,318 - getnode - INFO - nodesjob.py:609 - 开始写入输出文件...
2025-05-18 21:26:23,442 - getnode - INFO - nodesjob.py:618 - 成功生成订阅文件，总节点数: 13961
2025-05-18 21:26:23,579 - getnode - INFO - main.py:81 - 
=== 文件处理统计 ===
• 扫描文件总数: 4229
• 因大小跳过:   80 (1.9%)
• 有效处理文件: 4149
=== 节点处理统计 ===
• 扫描节点总数: 61591
• 节点去重数:   34224
• 真实节点数:   27367
