import requests
import json
import time
from dotenv import load_dotenv
import os

load_dotenv()  # åŠ è½½.envæ–‡ä»¶

# â†“â†“â†“ æ›¿æ¢æˆä½ çš„ä»¤ç‰Œ â†“â†“â†“
GITHUB_TOKEN = os.getenv("CRAWLER_GITHUB_TOKEN")  # ä»ç¯å¢ƒå˜é‡è·å–

# SEARCH_KEYWORDS = ["v2ray free", "vmess nodes", "ssrè®¢é˜…", "free proxies"]
SEARCH_KEYWORDS = ["v2ray free"]

def search_github():
    """è‡ªåŠ¨æœç´¢å«èŠ‚ç‚¹çš„ä»“åº“"""
    print("ğŸš€ æ­£åœ¨æœç´¢GitHubä»“åº“...")
    all_repos = []
    
    for keyword in SEARCH_KEYWORDS:
        page = 1
        while True:
            # æ„é€ æœç´¢URL
            url = f"https://api.github.com/search/repositories?q={keyword}&sort=updated&page={page}"
            headers = {"Authorization": f"Bearer {GITHUB_TOKEN}"}
            
            # å‘é€è¯·æ±‚
            response = requests.get(url, headers=headers)
            if response.status_code != 200:
                print(f"âš  é”™è¯¯ï¼š{response.status_code}ï¼Œå¯èƒ½è¾¾åˆ°APIé™åˆ¶")
                break
                
            data = response.json()
            if not data["items"]:
                break  # æ²¡æœ‰æ›´å¤šç»“æœ
                
            # æ”¶é›†ä»“åº“ä¿¡æ¯
            for repo in data["items"]:
                all_repos.append({
                    "name": repo["full_name"],
                    "url": repo["html_url"],
                    "description": repo["description"]
                })
            
            print(f"ğŸ“¦ å·²æ‰¾åˆ° {len(all_repos)} ä¸ªä»“åº“")
            page += 1
            time.sleep(1)  # é˜²æ­¢è¯·æ±‚è¿‡å¿«
            
    return all_repos

def find_nodes_in_repo(repo_url):
    """åœ¨ä»“åº“ä¸­æŸ¥æ‰¾èŠ‚ç‚¹æ–‡ä»¶"""
    print(f"\nğŸ” æ­£åœ¨æ‰«æä»“åº“ï¼š{repo_url}")
    api_url = repo_url.replace("https://github.com/", "https://api.github.com/repos/") + "/contents/"
    
    try:
        response = requests.get(api_url, headers={"Authorization": f"Bearer {GITHUB_TOKEN}"})
        files = response.json()
        
        # è¯†åˆ«å¯èƒ½çš„èŠ‚ç‚¹æ–‡ä»¶
        node_files = []
        for file in files:
            name = file["name"].lower()
            if name.endswith((".json", ".txt")) or "node" in name or "subscribe" in name:
                node_files.append(file["download_url"])
        
        return node_files
    except Exception as e:
        print(f"âŒ æ‰«æå¤±è´¥ï¼š{str(e)}")
        return []

def parse_node_file(file_url):
    """è§£æèŠ‚ç‚¹æ–‡ä»¶"""
    try:
        response = requests.get(file_url)
        content = response.text
        
        # å°è¯•è§£æJSONæ ¼å¼
        if file_url.endswith(".json"):
            return json.loads(content)
            
        # å¤„ç†vmess://ç­‰åè®®
        nodes = []
        for line in content.splitlines():
            line = line.strip()
            if line.startswith(("vmess://", "ss://", "trojan://")):
                nodes.append(line)
                
        return nodes
    except:
        return []

# ä¸»ç¨‹åº
if __name__ == "__main__":
    # æ­¥éª¤1ï¼šæœç´¢ä»“åº“
    repositories = search_github()
    
    # æ­¥éª¤2ï¼šéå†ä»“åº“æ”¶é›†èŠ‚ç‚¹
    all_nodes = []
    for repo in repositories[:3]:  # å…ˆæµ‹è¯•å‰3ä¸ªä»“åº“
        files = find_nodes_in_repo(repo["url"])
        
        for file_url in files:
            nodes = parse_node_file(file_url)
            if nodes:
                print(f"âœ… ä» {file_url.split('/')[-1]} æ‰¾åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
                all_nodes.extend(nodes)
    
    # ä¿å­˜ç»“æœ
    with open("nodes.json", "w") as f:
        json.dump(all_nodes, f, indent=2)
    print(f"\nğŸ‰ å®Œæˆï¼å…±æ”¶é›†åˆ° {len(all_nodes)} ä¸ªèŠ‚ç‚¹ï¼Œå·²ä¿å­˜åˆ° nodes.json")
